# new_sof
Atualmente, grande parte dos sensores utilizados em Internet das Coisas adota tecnologia sem fio, a fim de facilitar a construção de redes de sensoriamento. Neste sentido, a classificação do tipo de ambiente no qual estes sensores estão localizados exerce um importante papel no desempenho de tais redes de sensoriamento, uma vez que pode ser utilizada na determinação de níveis mais eficientes de consumo de energia dos sensores que as compõe. Assim, neste trabalho é apresentada a proposição de uma versão estendida do modelo classificador Fuzzy Auto-Organizável, que faz a classificação de ambientes internos a partir de medições do sinal de radiofrequência de uma rede de sensoriamento sem fio em um ambiente real. Foi realizada uma comparação do modelo de classificador original com o modelo proposto nesse trabalho, bem como outros métodos de aprendizado de máquina comuns na literatura. Como métricas foram avaliados: Acurácia média, F-Score, coeficiente Kappa e MSE. Os resultados experimentais mostram que a abordagem proposta obteve alto desempenho na solução do problema apresentado.

Os códigos desenvolvidos baseiam-se nos projetos como abaixo:

Fuzzy: No sistema de inferência Fuzzy (FIS) foi escolhido método Mamdani e o método max-min para fuzzificação, o método triangular para a defuzzificação e o tipo de função de associação é gaussiana. A função de associação e a base de regras são formadas usando o editor FIS. A partir da ferramenta gesfis no \emph{software} MATLAB foi gerado o sistema de inferência Fuzzy (FIS) do tipo Sugeno. A ferramenta extraiu um total de 243 regras da base de dados, para cada uma dessas regras existe um antecedente e um consequente , totalizando 243 antecedentes e consequentes. Existem 3 funções de pertinência triangulares para cada uma das 5 entradas. Foi utilizado valor mínimo para o operador \emph{AND} e valor máximo para o operador \emph{OR}. Foi especificado como método de implicação, para calcular o conjunto fuzzy consequente, o método do produto e como método de agregação foi utilizado somatório. E para método de defuzzificação foi utilizado média ponderada de todas as saídas das regras.

SVM Linear: Parâmetro de regularização=0.025; Kernel=Linear; Grau da função polinomial do kernel=3; Gama=Escalar; Encolhimento (\emph{shrinking})=Sim; Ativa estimativas de probabilidade=Não; Tolerância ao critério de parada=0.001; Tamanho do cache do kernel (em MB)=200; Maximo de iterações=Sem limites; Forma da função de decisão= \emph{One-vs-rest}; A semente do gerador de números são pseudo-aleatórios=Não.

SVM RBF: Parâmetro de regularização=1; Kernel=Linear; Grau da função polinomial do kernel=3; Gama=2; Encolhimento (\emph{shrinking})=Sim; Ativa estimativas de probabilidade=Não; Tolerância ao critério de parada=0.001; Tamanho do cache do kernel (em MB)=200; Máximo de iterações=Sem limites; Forma da função de decisão= \emph{One-vs-rest}; Semente do gerador de números são pseudo-aleatórios=Não.

Decision Tree: Função para medir a qualidade de uma divisão=Gini; Estratégia usada para escolher a divisão em cada nó=Melhor divisão; Profundidade máxima da árvore= 5; Número mínimo de amostras necessárias para dividir um nó interno=2; Número mínimo de amostras necessárias para estar em um nó folha=1; Fração ponderada mínima da soma total de pesos (de todas as amostras de entrada) necessária para estar em um nó folha=0; Número de recursos a serem considerados ao procurar a melhor divisão=Máximo de características; Gerador de números aleatórios= Randômicos; Máximo de nós de folha= Número ilimitado; Limiar para parada precoce no crescimento das árvores=$1^{-7}$; Pesos associados às classes= Todas as classes tem um peso; Parâmetro de complexidade usado para Poda de custo mínimo e complexidade=0.

Random Forest: Número de árvores na floresta=10; Função para medir a qualidade de uma divisão=Gini; Profundidade máxima da árvore=5; Número mínimo de amostras necessárias para dividir um nó interno=2; Número mínimo de amostras necessárias para estar em um nó folha=1; Fração ponderada mínima da soma total de pesos (de todas as amostras de entrada) necessária para estar em um nó folha=0; Número de recursos a serem considerados ao procurar a melhor divisão= 1; Máximo de nós de folha= Número ilimitado; Diminuição mínima da impureza=0; Limiar para parada precoce no crescimento das árvores=$1^{-7}$; Amostras de auto-inicialização são usadas na construção de árvores=Sim; Necessário usar amostras fora da bolsa para estimar a precisão da generalização=Não; Número de tarefas a serem executadas em paralelo=1; Controla a aleatoriedade da auto-inicialização das amostras usadas ao construir árvores=Sim; Pesos associados às classes= Todas as classes tem um peso; Parâmetro de complexidade usado para poda de custo mínimo e complexidade=0; Número de amostras a serem retiradas para treinar cada estimador de base= Todas as amostras.

Nearest Neighbors: Número de vizinhos a serem usados por padrão para consultas de vizinhos=3; Função de peso=Pesos uniformes; Algoritmo usado para calcular os vizinhos mais próximos=Auto(decide o algoritmo mais apropriado com base nos ajustes do método, pode ser \emph{BallTree}, \emph{KDTree} ou busca por força bruta); Tamanho da folha passado para o \emph{BallTree} ou \emph{KDTree}=30; Parâmetro $p$ para a métrica de Minkowski=2; Métrica da distância a ser usada na árvore=Minkowski; Número de tarefas a serem executadas em paralelo=1.

MLP-ANN: Tamanho da camada oculta=100; Função de ativação para a camada oculta=Função de unidade linear retificada; Solucionador para otimização de peso=Otimizador estocástico baseado em gradiente; Parâmetro de penalidade=1; Programação da taxa de aprendizado para atualizações de peso=Taxa de aprendizado constante igual a 0.001; Expoente da taxa de aprendizado de escala inversa=0.5; Número máximo de iterações=1000; Amostras devem ser embaralhadas em cada iteração=Sim; Tolerância para a otimização=$1^{-4}$; Momento para atualização de descida de gradiente=0.9; Se deve usar o impulso de Nesterov=Sim; A proporção de dados de treinamento a serem reservados como validação definida para parada antecipada=0.1; Beta 1=0.9; Beta 2=0.999; Epsilon= $1^{-8}$; Número máximo de épocas para não atender a melhoria=10.

AdaBoost: Número máximo de estimadores nos quais o aumento é finalizado=50; Taxa de aprendizado reduz a contribuição de cada classificador=1; Algoritmo=Algoritmo de reforço real SAMME.R.

Naive Bayes: Suavização=$1^{-9}$.

QDA: Limiar usado para estimativa de classificação=1^{-4}.

Obs: para rodar os códigos coloque tudo em uma mesma pasta.
