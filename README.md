# new_sof
Nowadays, a great part of the sensors adopted in IoT use wireless technology to facilitate the construction of sensor networks. In this sense, the classification of the type of environment in which these sensors are located plays an important role in the performance of these sensor networks, since it leads to efficient power consumption when operating the deployed IoT sensors. Thus, this dissertation presents an enhancement in the Self-Organizing Fuzzy Classifier model, which makes the classification of indoor environments from real-time measurements of the radio-frequency signal of a real wireless sensor network. A comparison between the original classifier model and the model proposed in this dissertation was made, as well as other common machine learning methods literature. The evaluated metrics were Accuracy, F-Score, Kappa coefficient, and MSE. The experimental results show that the proposed approach obtained high performance in solving the presented problem.
Os códigos desenvolvidos baseiam-se nos projetos como abaixo:

Fuzzy: No sistema de inferência Fuzzy (FIS) foi escolhido método Mamdani e o método max-min para fuzzificação, o método triangular para a defuzzificação e o tipo de função de associação é gaussiana. A função de associação e a base de regras são formadas usando o editor FIS. A partir da ferramenta gesfis no \emph{software} MATLAB foi gerado o sistema de inferência Fuzzy (FIS) do tipo Sugeno. A ferramenta extraiu um total de 243 regras da base de dados, para cada uma dessas regras existe um antecedente e um consequente , totalizando 243 antecedentes e consequentes. Existem 3 funções de pertinência triangulares para cada uma das 5 entradas. Foi utilizado valor mínimo para o operador \emph{AND} e valor máximo para o operador \emph{OR}. Foi especificado como método de implicação, para calcular o conjunto fuzzy consequente, o método do produto e como método de agregação foi utilizado somatório. E para método de defuzzificação foi utilizado média ponderada de todas as saídas das regras.

SVM Linear: Parâmetro de regularização=0.025; Kernel=Linear; Grau da função polinomial do kernel=3; Gama=Escalar; Encolhimento (\emph{shrinking})=Sim; Ativa estimativas de probabilidade=Não; Tolerância ao critério de parada=0.001; Tamanho do cache do kernel (em MB)=200; Maximo de iterações=Sem limites; Forma da função de decisão= \emph{One-vs-rest}; A semente do gerador de números são pseudo-aleatórios=Não.

SVM RBF: Parâmetro de regularização=1; Kernel=Linear; Grau da função polinomial do kernel=3; Gama=2; Encolhimento (\emph{shrinking})=Sim; Ativa estimativas de probabilidade=Não; Tolerância ao critério de parada=0.001; Tamanho do cache do kernel (em MB)=200; Máximo de iterações=Sem limites; Forma da função de decisão= \emph{One-vs-rest}; Semente do gerador de números são pseudo-aleatórios=Não.

Decision Tree: Função para medir a qualidade de uma divisão=Gini; Estratégia usada para escolher a divisão em cada nó=Melhor divisão; Profundidade máxima da árvore= 5; Número mínimo de amostras necessárias para dividir um nó interno=2; Número mínimo de amostras necessárias para estar em um nó folha=1; Fração ponderada mínima da soma total de pesos (de todas as amostras de entrada) necessária para estar em um nó folha=0; Número de recursos a serem considerados ao procurar a melhor divisão=Máximo de características; Gerador de números aleatórios= Randômicos; Máximo de nós de folha= Número ilimitado; Limiar para parada precoce no crescimento das árvores=$1^{-7}$; Pesos associados às classes= Todas as classes tem um peso; Parâmetro de complexidade usado para Poda de custo mínimo e complexidade=0.

Random Forest: Número de árvores na floresta=10; Função para medir a qualidade de uma divisão=Gini; Profundidade máxima da árvore=5; Número mínimo de amostras necessárias para dividir um nó interno=2; Número mínimo de amostras necessárias para estar em um nó folha=1; Fração ponderada mínima da soma total de pesos (de todas as amostras de entrada) necessária para estar em um nó folha=0; Número de recursos a serem considerados ao procurar a melhor divisão= 1; Máximo de nós de folha= Número ilimitado; Diminuição mínima da impureza=0; Limiar para parada precoce no crescimento das árvores=$1^{-7}$; Amostras de auto-inicialização são usadas na construção de árvores=Sim; Necessário usar amostras fora da bolsa para estimar a precisão da generalização=Não; Número de tarefas a serem executadas em paralelo=1; Controla a aleatoriedade da auto-inicialização das amostras usadas ao construir árvores=Sim; Pesos associados às classes= Todas as classes tem um peso; Parâmetro de complexidade usado para poda de custo mínimo e complexidade=0; Número de amostras a serem retiradas para treinar cada estimador de base= Todas as amostras.

Nearest Neighbors: Número de vizinhos a serem usados por padrão para consultas de vizinhos=3; Função de peso=Pesos uniformes; Algoritmo usado para calcular os vizinhos mais próximos=Auto(decide o algoritmo mais apropriado com base nos ajustes do método, pode ser \emph{BallTree}, \emph{KDTree} ou busca por força bruta); Tamanho da folha passado para o \emph{BallTree} ou \emph{KDTree}=30; Parâmetro $p$ para a métrica de Minkowski=2; Métrica da distância a ser usada na árvore=Minkowski; Número de tarefas a serem executadas em paralelo=1.

MLP-ANN: Tamanho da camada oculta=100; Função de ativação para a camada oculta=Função de unidade linear retificada; Solucionador para otimização de peso=Otimizador estocástico baseado em gradiente; Parâmetro de penalidade=1; Programação da taxa de aprendizado para atualizações de peso=Taxa de aprendizado constante igual a 0.001; Expoente da taxa de aprendizado de escala inversa=0.5; Número máximo de iterações=1000; Amostras devem ser embaralhadas em cada iteração=Sim; Tolerância para a otimização=$1^{-4}$; Momento para atualização de descida de gradiente=0.9; Se deve usar o impulso de Nesterov=Sim; A proporção de dados de treinamento a serem reservados como validação definida para parada antecipada=0.1; Beta 1=0.9; Beta 2=0.999; Epsilon= $1^{-8}$; Número máximo de épocas para não atender a melhoria=10.

AdaBoost: Número máximo de estimadores nos quais o aumento é finalizado=50; Taxa de aprendizado reduz a contribuição de cada classificador=1; Algoritmo=Algoritmo de reforço real SAMME.R.

Naive Bayes: Suavização=$1^{-9}$.

QDA: Limiar usado para estimativa de classificação=1^{-4}.

O software SOF
